# -*- coding: utf-8 -*-
"""Traffic_Sign_Classifier_P3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16UU5wx_UPs9JTRMiJrnoji_ylRHuY1Af
"""

!pip uninstall tensorflow

!pip install tensorflow==1.15

# Commented out IPython magic to ensure Python compatibility.
# Load pickled data
import pickle
### Data exploration visualization code goes here.
### Feel free to use as many code cells as needed.
import random
import numpy as np
import matplotlib.pyplot as plt
import cv2
from sklearn.utils import shuffle
import tensorflow as tf
from tensorflow.contrib.layers import flatten
# Visualizations will be shown in the notebook.
# %matplotlib inline

# TODO: Fill this in based on where you saved the training and testing data
training_file = "/content/drive/MyDrive/Colab Notebooks/train.p"
validation_file="/content/drive/MyDrive/Colab Notebooks/valid.p"
testing_file = "/content/drive/MyDrive/Colab Notebooks/test.p"

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)
    
X_train, y_train = train['features'], train['labels']
X_valid, y_valid = valid['features'], valid['labels']
X_test, y_test = test['features'], test['labels']

### Replace each question mark with the appropriate value. 
### Use python, pandas or numpy methods rather than hard coding the results

# TODO: Number of training examples
n_train = len(X_train)

# TODO: Number of validation examples
n_validation = len(X_valid)

# TODO: Number of testing examples.
n_test = len(X_test)

# TODO: What's the shape of an traffic sign image?
image_shape = np.shape(X_train[0])

# TODO: How many unique classes/labels there are in the dataset.
n_classes = 43

print("Number of training examples =", n_train)
print("Number of testing examples =", n_test)
print("Image data shape =", image_shape)
print("Number of classes =", n_classes)

index = random.randint(0, n_test)
image = X_train[index].squeeze()

plt.figure(figsize = (1,1))
plt.imshow(image)
print(y_train[index])

### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include 
### converting to grayscale, etc.
### Feel free to use as many code cells as needed.
# The format of the data is (Set count, width, height, channel)
# Pad the 28*28 size image by 2 pixels in width and height.
X_train = np.pad(X_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')
X_valid = np.pad(X_valid, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')
X_test = np.pad(X_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')

def normalize(data):
    return data / 255.0

X_train = normalize(X_train) 
X_test = normalize(X_test) 
X_valid = normalize(X_valid)

### Define your architecture here.
### Feel free to use as many code cells as needed.
# training part
# Hyperparameter
EPOCHS = 50
BATCH_SIZE = 128
keep_prob = 0.8

# LeNet function
def LeNet(x):
    mu = 0
    sigma = 0.1

    conv1_out = 6
    conv2_out = 16
    conv3_out = 48
    fc1_in, fc1_out = 6*6*conv3_out, 256
    fc2_out = 128
    fc3_out = 84
 
    # ConvNet 1st layer
    # weight(filter): 5*5, Input 3 channel, output 6 layers
    conv1_W = tf.Variable(tf.truncated_normal(shape = (5, 5, 3, conv1_out), mean = mu, stddev = sigma), name = "w1")
    conv1_b = tf.Variable(tf.zeros(conv1_out))
    conv1 = tf.nn.conv2d(x, conv1_W, strides = [1, 1, 1, 1], padding = 'VALID') + conv1_b
 
    # convolution padding
    conv1 = tf.nn.relu(conv1)
    # max pooling - No depth change
    conv1 = tf.nn.max_pool(conv1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding='VALID')
    
    # ConvNet 2nd layer - 1*1 convolution
    # weight(filter): 5*5, Input 6 channel, output 16 layers
    conv2_W = tf.Variable(tf.truncated_normal(shape = (1, 1, conv1_out, conv2_out), mean = mu, stddev = sigma))
    conv2_b = tf.Variable(tf.zeros(conv2_out))
    conv2 = tf.nn.conv2d(conv1, conv2_W, strides = [1, 1, 1, 1], padding = 'VALID') + conv2_b
 
    conv2 = tf.nn.relu(conv2)
    # max pooling - No depth change

    # ConvNet 3rd layer
    # weight(filter): 5*5, Input 16 channel, output 48 layers
    conv3_W = tf.Variable(tf.truncated_normal(shape = (5, 5, conv2_out, conv3_out), mean = mu, stddev = sigma))
    conv3_b = tf.Variable(tf.zeros(conv3_out))
    conv3 = tf.nn.conv2d(conv2, conv3_W, strides = [1, 1, 1, 1], padding = 'VALID') + conv3_b
 
    conv3 = tf.nn.relu(conv3)
    # max pooling - No depth change
    conv3 = tf.nn.max_pool(conv3, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding='VALID')

    # Make conv3 output flat (flatten) with shape (128, 6*6*48 = 1748)
    fc0 = flatten(conv3)

    # Full Connnected Layers
    fc1_W = tf.Variable(tf.truncated_normal(shape = (fc1_in, fc1_out), mean = mu, stddev = sigma))
    fc1_b = tf.Variable(tf.zeros(fc1_out))
    fc1 = tf.matmul(fc0, fc1_W) + fc1_b
 
    fc1 = tf.nn.relu(fc1)
    # fc1 = tf.nn.dropout(fc1, keep_prob)

    fc2_W = tf.Variable(tf.truncated_normal(shape = (fc1_out, fc2_out), mean = mu, stddev = sigma))
    fc2_b = tf.Variable(tf.zeros(fc2_out))
    fc2 = tf.matmul(fc1, fc2_W) + fc2_b
 
    fc2 = tf.nn.relu(fc2)
    # fc2 = tf.nn.dropout(fc2, keep_prob)

    fc3_W = tf.Variable(tf.truncated_normal(shape = (fc2_out, fc3_out), mean = mu, stddev = sigma))
    fc3_b = tf.Variable(tf.zeros(fc3_out))
    fc3 = tf.matmul(fc2, fc3_W) + fc3_b
 
    fc3 = tf.nn.relu(fc3)
    # fc3 = tf.nn.dropout(fc3, keep_prob)

    fc4_W = tf.Variable(tf.truncated_normal(shape = (fc3_out, n_classes), mean = mu, stddev = sigma))
    fc4_b = tf.Variable(tf.zeros(n_classes))
    logits = tf.matmul(fc3, fc4_W) + fc4_b
 
    return logits

x = tf.placeholder(tf.float32, (None, 36, 36, 3))
y = tf.placeholder(tf.int32, (None))

# The process of modifying the weights with the result of the LeNet algorithm and one_hot_y.
# Tensorflow 2 uses the following courses as a function instead of sessions.
one_hot_y = tf.one_hot(y, n_classes)
 
rate = 0.001
logits = LeNet(x)

cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = one_hot_y)
loss_operation = tf.reduce_mean(cross_entropy)
optimizer = tf.train.AdamOptimizer(learning_rate = rate)
training_operation = optimizer.minimize(loss_operation)
 
# Evaluate accuracy by entering validation data in the accuracy assessment course session.
correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))
accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

 
def evaluate(X_data, y_data):
    num_examples = len(X_data)
    total_accuracy = 0
    sess = tf.get_default_session()
    for offset in range(0, num_examples, BATCH_SIZE):
        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]
        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})
        total_accuracy += (accuracy * len(batch_x))
    return total_accuracy / num_examples

# Run session
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    num_examples = len(X_train)
    
    print("Training...")
    print()
    for i in range(EPOCHS):
        X_train, y_train = shuffle(X_train, y_train)
        for offset in range(0, num_examples, BATCH_SIZE):
            end = offset + BATCH_SIZE
            batch_x, batch_y = X_train[offset:end], y_train[offset:end]
            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})
 
        validation_accuracy = evaluate(X_valid, y_valid)
        print("EPOCH {} ...".format(i + 1))
        print("Validation Accuracy = {}".format(validation_accuracy))
        print()

    try:
        saver
    except NameError:
        saver = tf.train.Saver()
    saver.save(sess, 'traffic_classi')
    test_accuracy = evaluate(X_test, y_test)
    print("Test Accuracy = {}".format(test_accuracy))
    print("Model saved")

with tf.Session() as sess:
    saver.restore(sess, tf.train.latest_checkpoint('.')) 
    predicted_logits = sess.run(logits, feed_dict={x: X_test})
    predicted_labels = np.argmax(predicted_logits, axis=1)
    test_accuracy = evaluate(X_test, y_test)
    print("Test Accuracy = {:.0f}%".format(test_accuracy*100))

plt.rcdefaults()
import os
import matplotlib.image as mpimg

X_final_test = []
X_final_test_name = []

def pipeline(img):
    global X_final_test
    
    #Resize for consistency
    resize_image = cv2.resize(img, (32, 32))

    #Convert to 3 channels
    processed_img = cv2.cvtColor(resize_image, cv2.COLOR_BGRA2BGR)
    processed_img = np.pad(processed_img, ((2, 2), (2, 2), (0, 0)), 'constant')
    print(np.shape(processed_img))
    
    X_final_test.append(processed_img)
    return resize_image

#printing out some stats and plotting
plt.close("all")
for file in sorted(os.listdir("/content/drive/MyDrive/Colab Notebooks/test_images/")):
    plt.figure(figsize=(1,1))
    plt.title(file)
    plt.imshow(pipeline(mpimg.imread('/content/drive/MyDrive/Colab Notebooks/test_images/' + file)))    
    plt.figure()
    plt.title(file + "(Original)")
    plt.imshow(mpimg.imread('/content/drive/MyDrive/Colab Notebooks/test_images/' + file))
    X_final_test_name += [file]    
    
X_final_graph = X_final_test

with tf.Session() as sess:
    saver.restore(sess, tf.train.latest_checkpoint('.'))  
    predicted_logits = sess.run(logits, feed_dict={x: X_final_test})
    predicted_labels = np.argmax(predicted_logits, axis=1)
    for i in range(len(X_final_test)):
        print("Prediction: {}".format(predicted_labels[i]))
    y_final_test = [17,13,17,17,14,4,38]
    test_accuracy = evaluate(X_final_test, y_final_test)
    print("Test Accuracy = {:.0f}%".format(test_accuracy*100))

